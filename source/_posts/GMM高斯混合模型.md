---
title: GMMé«˜æ–¯æ··åˆæ¨¡å‹
abbrlink: 6d95358d
date: 2023-10-20 20:53:10
tags:
cover: 
---

<!-- more -->
## 1. GMMé«˜æ–¯æ··åˆæ¨¡å‹åŸç†
### 1.1 æ··åˆå¯†åº¦æ¨¡å‹
å¤æ‚çš„æ¦‚ç‡å¯†åº¦å‡½æ•°å¯ä»¥ç”±ç®€å•å¯†åº¦å‡½æ•°çº¿æ€§ç»„åˆæ„æˆ
$$p(x|\theta) = {\sum\limits_{k=1}^{M} a_{k}p_{k}(x|\theta_k)},\quad a_{k}>0,\quad \sum\limits_{k=1}^{M} a_{k}=1$$
GMM æ˜¯æ··åˆå¯†åº¦æ¨¡å‹çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œç”±å¤šä¸ªé«˜æ–¯ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰å‡½æ•°çš„ç»„åˆæ„æˆ
$$p(x) = \sum\limits_{k=1}^{M} a_{k}N(x;\mu_k,\Sigma_k)$$
#### ==ä»¥ä¸‹3ç‚¹å¯¹äºç†è§£GMMæ¨¡å‹çš„é¢„æµ‹å¾ˆé‡è¦ ï¼ï¼ï¼==
- è¿™é‡Œçš„$p(x)$æ˜¯ä¼¼ç„¶å‡½æ•°ï¼Œå®Œæ•´è¡¨è¾¾åº”è¯¥æ˜¯$p(x|\omega_i)$ï¼Œè¿™é‡Œçš„$\omega_i$è¡¨ç¤ºæ˜¯ç¬¬$i$ç±»æ•°æ®ï¼Œå³è®­ç»ƒGMMå…¶å®å¯¹æ¯ä¸€ç±»éƒ½å•ç‹¬å­¦ä¹ ä¸€ä¸ªGMMæ¨¡å‹ã€‚
- æ¯”å¦‚ï¼šåœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œå¦‚10åˆ†ç±»é—®é¢˜ã€‚
    - é¦–å…ˆå­¦ä¹ åˆ°10ä¸ªä¸åŒçš„GMMæ¨¡å‹ï¼ˆå³ä¼¼ç„¶å‡½æ•°å…¬å¼ï¼‰ï¼›
    - é€šè¿‡é¢„æµ‹æ ·æœ¬å¸¦å…¥ä¼¼ç„¶å‡½æ•°å…¬å¼ï¼Œå¾—åˆ°æ¦‚ç‡å¯†åº¦å€¼ï¼›
    - å†ä¹˜ä»¥ä¸åŒç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡$p(\omega_i)$ï¼Œå¾—åˆ°10ä¸ªå¯¹åº”é¢„æµ‹æ ·æœ¬çš„åéªŒæ¦‚ç‡$p(\omega_i|x)$ï¼›
    - å–è¿™10ä¸ªå€¼é‡Œæœ€å¤§çš„æ‰€å¯¹åº”çš„ç±»åˆ«ä½œä¸ºè¿™ä¸ªæ ·æœ¬çš„ç±»åˆ«ã€‚
- è¿™ä¸€åˆ‡é¢„æµ‹éƒ½æ˜¯åŸºäºè´å¶æ–¯å…¬å¼ï¼š
$$P(\omega_i|x)={\dfrac{p(x|\omega_i)P(\omega_i)}{p(x)}}$$
### 1.2 GMMçš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆå‚æ•°ä¼°è®¡ï¼‰
GMM éœ€è¦ä¼°è®¡çš„å‚æ•°:
$$\theta=(\alpha_1,\dots,\alpha_M,\mu_1,\dots,\mu_M,\Sigma_1,\dots,\Sigma_M)$$
å¯¹æ•°ä¼¼ç„¶å‡½æ•°:
$$l({\theta})=\sum\limits_{i=1}^nln\left[\sum\limits_{k=1}^M a_{k}N(x_i;\mu_k,\Sigma_k)\right]$$
- æå€¼ç‚¹æ–¹ç¨‹æ˜¯å¤æ‚çš„è¶…è¶Šæ–¹ç¨‹ç»„ï¼Œå¾ˆéš¾ç›´æ¥æ±‚è§£
- å¸¸ç”¨çš„ GMM å‚æ•°ä¼°è®¡æ–¹æ³•æ˜¯ EM ç®—æ³•

è®­ç»ƒé›†å’Œå¾…å­¦ä¹ å‚æ•°
- è®­ç»ƒæ•°æ®é›†:  $D_X = \{X_1,\dots,X_n\}$
- å­¦ä¹ å‚æ•°: $\theta=\{\alpha_k,\mu_k,\Sigma_k\}_{k=1,\dots,M}$

å­˜åœ¨çš„é—®é¢˜
- åªæœ‰æ ·æœ¬ $x_i$ ï¼Œä½†ä¸çŸ¥é“æ˜¯ç”±å“ªä¸€ä¸ªæˆä»½é«˜æ–¯äº§ç”Ÿçš„
- ä»¤ $y_i=k$è¡¨ç¤º $x_i$æ˜¯ç”±ç¬¬$k$ä¸ªæˆä»½é«˜æ–¯äº§ç”Ÿï¼Œæ„é€ é›†åˆï¼š
$$D_Y=\{y_1,\dots,y_n\}$$
- å®Œæ•´çš„æ•°æ®é›† $D=D_X\cup D_Y$ï¼Œå…¶ä¸­ $D_Y$ä¸ºç¼ºå¤±çš„æ•°æ®

## 2. EMç®—æ³•
- ==é‡è¦æ€»ç»“ï¼ï¼ï¼==
å¯¹äº$D_Y=\{y_1,\dots,y_n\}$,ç›¸å¯¹äºæ­¦æ–­çš„å°†æ¯ä¸ªè®­ç»ƒæ ·æœ¬ç¡®å®šä¸ºæŸä¸ªé«˜æ–¯äº§ç”Ÿçš„ï¼Œæ›´å¥½çš„åšæ³•æ˜¯è®¡ç®—æ¯ä¸ªæ ·æœ¬ç”±æ¯ä¸ªé«˜æ–¯äº§ç”Ÿçš„æ¦‚ç‡ï¼Œåæ–‡çš„ä»£ç å®ç°ä¹Ÿæ˜¯åŸºäºè¿™ä¸€ç‚¹çš„ï¼Œå…¶ä¸­$pdfs$ä¸º$m*k$ç»´çŸ©é˜µï¼Œæ¯è¡Œä¸ºè¯¥æ ·æœ¬ç”±ä¸åŒé«˜æ–¯äº§ç”Ÿçš„æ¦‚ç‡ï¼Œå…±æœ‰mä¸ªæ ·æœ¬ã€‚
### 2.1 Eæ­¥ï¼ˆExpectationï¼‰
- éšå˜é‡çš„æ¦‚ç‡ä¼°è®¡

$$P(y_i=k|x_i)=\dfrac{P(y_i=k)P(x_i|y_i=k)}{P(x_i)}=\dfrac{a_{k}N(x_i;\mu_k,\Sigma_k)}{\sum\limits_{j=1}^M a_{j}N(x_i;\mu_j,\Sigma_j)} \tag{1}$$

### 2.2 Mæ­¥ï¼ˆMaximizeï¼‰
- æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒå‚æ•°ä¹Ÿéœ€è¦ç”±æ‰€æœ‰æ ·æœ¬å‚ä¸ä¼°è®¡ï¼ŒåŒæ—¶éœ€è¦
- è€ƒè™‘æ ·æœ¬ç”±ä¸åŒé«˜æ–¯äº§ç”Ÿçš„æ¦‚ç‡
$$\hat{a}_k=\dfrac{1}{n}\sum\limits_{i=1}^nP(y_i=k) \tag{2}$$$$\hat{\mu}_k=\dfrac{\sum\limits_{i=1}^nP(y_i=k)x_i}{\sum\limits_{i=1}^nP(y_i=k)} \tag{3}$$$$\hat{\Sigma}_k=\dfrac{\sum\limits_{i=1}^nP(y_i=k)(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^t}{\sum\limits_{i=1}^nP(y_i=k)} \tag{4}$$

## 3. ç®—æ³•
---
Input: è®­ç»ƒé›†$D_X = \{X_1,\dots,X_n\}$ï¼Œé«˜æ–¯æ•°$M$
Output: GMMçš„æ¨¡å‹å‚æ•° $\theta$
1. éšæœºåˆå§‹åŒ–$\theta$;
2. repeat
    Eæ­¥ï¼šå…¬å¼(1)ä¼°è®¡æ ·æœ¬ç”±ä¸åŒé«˜æ–¯äº§ç”Ÿçš„æ¦‚ç‡ï¼›
    Mæ­¥ï¼šå…¬å¼(2-4)é‡æ–°ä¼°è®¡æ¨¡å‹çš„å‚æ•°$\theta$ï¼›
3. until è¾¾åˆ°æ”¶æ•›ç²¾åº¦
---
ç±»æ¯”KMeansç®—æ³•ï¼š
- Eæ­¥æ˜¯åœ¨å·²çŸ¥è´¨å¿ƒä½ç½®çš„æ¡ä»¶ä¸‹ï¼ŒæŠŠå„ç‚¹èšç±»åˆ°æœ€è¿‘çš„è´¨å¿ƒï¼›
- Mæ­¥æ˜¯æ ¹æ®ç±»å†…å„ç‚¹ï¼Œè°ƒæ•´è´¨å¿ƒä½ç½®ã€‚

## 4. å®éªŒåŠä»£ç 
### 4.1 å®éªŒå†…å®¹
1.	ä½¿ç”¨Pythonç¼–ç¨‹å®ç°GMMç®—æ³•ï¼šè¦æ±‚ç‹¬ç«‹å®Œæˆç®—æ³•ç¼–ç¨‹ï¼Œç¦æ­¢è°ƒç”¨å·²æœ‰å‡½æ•°åº“æˆ–å·¥å…·ç®±ä¸­çš„å‡½æ•°ï¼›
2.	ä½¿ç”¨ä»¿çœŸæ•°æ®æµ‹è¯•ç¨‹åºçš„æ­£ç¡®æ€§ï¼š
    1. ä¸¤ç±»2ç»´å„1000ä¸ªè®­ç»ƒæ ·æœ¬ï¼›
    2. 	æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬åˆ†åˆ«é‡‡æ ·è‡ªåŒ…å«2ä¸ªåˆ†é‡çš„é«˜æ–¯æ··åˆæ¨¡å‹ï¼Œåˆ†åˆ«ä½¿ç”¨ä¸¤ä¸ªç±»åˆ«çš„è®­ç»ƒæ ·æœ¬å­¦ä¹ æ¨¡å‹åŒ–æ¯ä¸ªç±»åˆ«æ¡ä»¶æ¦‚ç‡å¯†åº¦çš„GMMæ¨¡å‹å‚æ•°ï¼Œå¹¶ä¸æ¨¡å‹çš„çœŸå®å€¼æ¯”è¾ƒï¼›
    3.	å‡è®¾ä¸¤ä¸ªç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ç›¸ç­‰ï¼Œä½¿ç”¨å­¦ä¹ åˆ°çš„æ¨¡å‹å‚æ•°åˆ†ç±»æµ‹è¯•æ ·æœ¬æ•°æ®ï¼Œç»Ÿè®¡åˆ†ç±»çš„æ­£ç¡®ç‡
3.	MNISTæ•°æ®é›†æµ‹è¯•ï¼š
    1.	MNIST-Train-Samples.csvä¸­åŒ…å«30000ä¸ª17ç»´ç‰¹å¾æ‰‹å†™æ•°å­—æ ·æœ¬è®­ç»ƒï¼ŒMNIST-Train-Labels.csvä¸­åŒ…å«è®­ç»ƒæ ·æœ¬çš„æ ‡ç­¾ï¼›
    2.	åˆ†åˆ«ä½¿ç”¨æ¯ä¸ªæ•°å­—çš„æ ·æœ¬é›†å­¦ä¹ è¯¥ç±»åˆ«çš„GMMæ¨¡å‹å‚æ•°ï¼›
    3.	ä½¿ç”¨10ä¸ªç±»åˆ«GMMæ¨¡å‹æ„æˆçš„åˆ†ç±»å™¨ï¼Œåˆ†ç±»MNIST-Test-Samples.csvä¸­çš„10000ä¸ªæ ·æœ¬ï¼Œä¸MNIST-Test-Labels.csvçš„ç±»åˆ«æ ‡è®°æ¯”å¯¹ï¼Œè®¡ç®—è¯†åˆ«çš„æ­£ç¡®ç‡ï¼›
    4.	å°è¯•è®¾ç½®ä¸åŒçš„GMMæ¨¡å‹è¶…å‚æ•°â€”é«˜æ–¯æ•°ï¼Œæµ‹è¯•ä¸åŒé«˜æ–¯æ•°çš„GMMåˆ†ç±»å™¨çš„è¯†åˆ«æ­£ç¡®ç‡ï¼›

### 4.2 ç¨‹åºä»£ç 
#### 4.2.1 å¯¼å…¥åº“å¹¶åˆ¶ä½œå·¥å…·å‡½æ•°
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
import pandas as pd
import random
import warnings
import math
import copy
warnings.filterwarnings("ignore")

# ç»˜åˆ¶æ¤­åœ†å‚è€ƒä»£ç 
def plot_cov_ellipse(cov, pos, nstd=2, ax=None):
    def eigsorted(cov):
        vals, vecs = np.linalg.eigh(cov)
        order = vals.argsort()[::-1]
        return vals[order], vecs[:, order]
 
    if ax is None:
        ax = plt.gca()
 
    vals, vecs = eigsorted(cov)
    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))
 
    # Width and height are "full" widths, not radius
    width, height = 2 * nstd * np.sqrt(vals)
    ellip = Ellipse(xy=pos, width=width, height=height, facecolor='#c8e0e4', angle=theta)
 
    ax.add_artist(ellip)
    return ellip
# ç»˜å›¾å‡½æ•° åœ¨äºŒç»´ç‰¹å¾æ—¶å¯ç”¨
def plot(data, mu, covariance, class_label):
    plt.scatter(data[:, 0], data[:, 1], c=class_label)
    n_components = len(mu)
    for j in range(n_components):
        plot_cov_ellipse(covariance[j], mu[j])
    plt.show()
# å®šä¹‰é«˜æ–¯å‡½æ•°
def Gaussian(x, mu, covariance):
    ret = []
    for i in x:
        X, mu, covariance = np.array(i), np.array(mu), np.array(covariance)
        ret.append( np.exp(-0.5 * (np.dot(np.dot((X-mu).T, np.linalg.inv(covariance)), X-mu))) /
                    math.sqrt(np.linalg.det(2.0 * math.pi * covariance)))
    return np.array(ret)

```
#### 4.2.2 GMMé¢å‘å¯¹è±¡å®ç°
``` python
# GMMä¸»ä½“
class GMM:
    def __init__(self, X, y, n_components, iteration=40, eps=1e-9):
        self.X = X
        self.y = y
        self.m, self.n = X.shape
        # ç»„æ•°
        self.n_components = n_components
        self.iteration = iteration
        self.eps = eps
        # é«˜æ–¯æ··åˆæ¨¡å‹å‚æ•°
        self.alpha = np.ones(n_components) * 1 / n_components
        self.mu = np.random.random((self.n_components, self.n))
        self.covariance = np.zeros((self.n_components, self.n, self.n))
        # 1. éšæœºåˆå§‹åŒ– mu ä¸ºæ ·æœ¬éšæœºç‚¹
        # mu n_components * 2
        row_rand_array = np.arange(X.shape[0])
        np.random.shuffle(row_rand_array)
        self.mu = X[row_rand_array[0:n_components]]
        
        
        # 2. åˆå§‹åŒ– cov alpha
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬åˆ° åˆå§‹åŒ–ä¸­å¿ƒç‚¹ çš„è·ç¦» 
        distance = np.tile(np.sum(X * X, axis=1).reshape((self.m, 1)), (1, self.n_components)) + np.tile(np.sum(self.mu * self.mu, axis=1).T,(self.m, 1)) - 2 * np.dot(X, self.mu.T)
        # åˆå§‹æ ‡ç­¾ æŒ‰ç…§è·ç¦»åˆ†ç±»
        orginial_labels = np.argmin(distance, axis=1)
        for i in range(self.n_components):
            temp = X[orginial_labels == i, :]
            self.alpha[i] = temp.shape[0] / self.m
            self.covariance[i, :, :] = np.cov(temp.T)
            
        # 3. æ¦‚ç‡å¯†åº¦ æ¨æ–­æ ·æœ¬ ğ±ğ‘– ç”±æ¯ä¸€ä¸ªé«˜æ–¯äº§ç”Ÿçš„æ¦‚ç‡
        # m * n_components 
        # ç¬¬iè¡Œ ä¸º æ ·æœ¬ ğ±ğ‘– ç”±æ¯ä¸€ä¸ªé«˜æ–¯äº§ç”Ÿçš„æ¦‚ç‡
        self.pdfs = np.zeros((self.m, self.n_components))
        
    def Train(self):
        nowIter = 0
        preLogLikelihood = self.LogLikelihood()
        while nowIter < self.iteration:
            if nowIter % 20 == 0:
                print("Iter:", nowIter)
            self.Expectation()
            self.Maximize()
#             plot(self.X, self.mu, self.covariance, self.y)
            nowIter += 1
            logLikelihood = self.LogLikelihood()
            if abs(logLikelihood - preLogLikelihood) < self.eps:
                break
            preLogLikelihood = logLikelihood
            
            
            
    # æ ¹æ®å½“å‰çš„å„ä¸ªç»„åˆ† alphaã€mu å’Œ covariance è®¡ç®—å¯¹æ•°ä¼¼ç„¶å‡½æ•°å€¼
    def LogLikelihood(self):
        for j in range(self.n_components):
            self.pdfs[:, j] = self.alpha[j] * Gaussian(self.X, self.mu[j], self.covariance[j])
        return np.mean(np.log(np.sum(self.pdfs, axis=1)))
        
    def Expectation(self):
        # Eæ­¥
        for k in range(self.n_components):
            self.pdfs[:,k] = self.alpha[k] * Gaussian(self.X, self.mu[k], self.covariance[k])
            self.W = self.pdfs / np.sum(self.pdfs,axis=1).reshape(-1,1)
        
    def Maximize(self):
        # Mæ­¥
        self.alpha = np.sum(self.W, axis=0) / np.sum(self.W)
        for k in range(self.n_components):
            self.mu[k] = np.average(self.X, axis=0, weights=self.W[:, k])
            cov = 0
            for i in range(self.m):
                temp = (self.X[i, :] - self.mu[k, :]).reshape(-1, 1)
                cov += self.W[i, k] * np.dot(temp, temp.T)
            self.covariance[k, :, :] = cov / np.sum(self.W[:, k]) 
```

#### 4.2.3 Emuæ•°æ®æµ‹è¯•
**å‚æ•°ä¼°è®¡**
```python
train_X = pd.read_csv(r'Emu-Train-Samples.csv', header=None)
train_Y = pd.read_csv(r'Emu-Train-Labels.csv', header=None)
train = copy.deepcopy(train_X)
train["label"] = train_Y
train = train.to_numpy()
train_1_X = np.array([[i[0], i[1]] for i in train if i[2] == 0])
train_1_Y = np.array([i[2] for i in train if i[2] == 0])
train_2_X = np.array([[i[0], i[1]] for i in train if i[2] == 1])
train_2_Y = np.array([i[2] for i in train if i[2] == 1])
plt.scatter(train_1_X[:, 0], train_1_X[:, 1], c='blue')
plt.scatter(train_2_X[:, 0], train_2_X[:, 1], c='green')

myGMM1 = GMM(train_1_X, train_1_Y, 2)
myGMM1.Train()
myGMM2 = GMM(train_2_X, train_2_Y, 2)
myGMM2.Train()

print("Class_1:")
print("mu:")
print(myGMM1.mu)
print("alpha:")
print(myGMM1.alpha)
print("covariance:")
print(myGMM1.covariance)

print("Class_2:")
print("mu:")
print(myGMM2.mu)
print("alpha:")
print(myGMM2.alpha)
print("covariance:")
print(myGMM2.covariance)
```

**é¢„æµ‹**
```python
test_X = pd.read_csv(r'Emu-Test-Samples.csv', header=None).to_numpy()
test_Y = pd.read_csv(r'Emu-Test-Labels.csv', header=None).to_numpy().reshape(-1)

def Likelihood(X, alpha, mu, covariance, n_components):
    pdfs = np.zeros((len(X), n_components))
    for k in range(n_components):
        pdfs[:, k] = alpha[k] * Gaussian(X, mu[k], covariance[k])
    return np.sum(pdfs, axis=1)

pdfs_1 = Likelihood(test_X, myGMM1.alpha, myGMM1.mu, myGMM1.covariance, len(myGMM1.alpha))
pdfs_2 = Likelihood(test_X, myGMM2.alpha, myGMM2.mu, myGMM2.covariance, len(myGMM2.alpha))

predict = []
for i in range(len(test_X)):
    if(pdfs_1[i] > pdfs_2[i]):
        predict.append(0)
    else:
        predict.append(1)

accuracy = sum(predict == test_Y) / len(predict)
print(accuracy)
```


#### 4.2.4 MNISTæ•°æ®é›†æµ‹è¯•
```python
train_mnist_X = pd.read_csv(r'MNIST-Train-Samples.csv', header=None)
train_mnist_Y = pd.read_csv(r'MNIST-Train-Labels.csv', header=None)
test_mnist_X = pd.read_csv(r'MNIST-Test-Samples.csv', header=None)
test_mnist_Y = pd.read_csv(r'MNIST-Test-Labels.csv', header=None)
train_mnist = train_mnist_X
train_mnist["label"] = train_mnist_Y
train_mnist = train_mnist.to_numpy()
test_mnist_X = test_mnist_X.to_numpy()
test_mnist_Y = test_mnist_Y.to_numpy().reshape(-1)

def GmmOfMnist(number):
    alphas = []
    mus = []
    covariances = []
    pdfs = []

#     print("Train Begin!")
    for index in range(0, 10):
        print("GMM", number," ", "class", index)
        nowTrain_X = np.array([i[:-1] for i in train_mnist if i[-1] == index])
#         print(np.shape(nowTrain_X))
        nowTrain_Y = [index] * len(nowTrain_X)
        nowGMM = GMM(nowTrain_X, nowTrain_Y, number)
        nowGMM.Train()
        alphas.append(nowGMM.alpha)
        mus.append(nowGMM.mu)
        covariances.append(nowGMM.covariance)
        
#     print("Test Begin!")
    for i in range(0, 10):  
        pdfs.append(Likelihood(test_mnist_X, alphas[i], mus[i], covariances[i], len(alphas[i])))
    result = np.argmax(pdfs, axis = 0)
    accuracy_num = sum(result == test_mnist_Y) 
    accuracy = accuracy_num / len(result)
    return accuracy_num, accuracy

for i in range(2, 6):
    accuracy_num, accuracy = GmmOfMnist(i)
    print("GMM number:", i)
    print("æ­£ç¡®è¯†åˆ«æ•°: ", accuracy_num)
    print("æ­£ç¡®è¯†åˆ«ç‡: ", accuracy)
```

## 5. é‡åˆ°çš„é—®é¢˜åŠè§£å†³æ–¹æ³•
- é«˜æ–¯å‡å€¼åˆå§‹åŒ–é—®é¢˜ï¼š
    åœ¨å‰å‡ è½®è®­ç»ƒä¸­æœ‰å‡ æ¬¡æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£ï¼Œå³æœ‰ä¸ªåˆ«é«˜æ–¯æ¨¡å‹é‡å è¦†ç›–äº†åŒä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæˆ–è€…ä¸€ä¸ªé«˜æ–¯å®Œå…¨è¦†ç›–äº†å¦ä¸€ä¸ªé«˜æ–¯çš„æ ·æœ¬èŒƒå›´ã€‚è¿™æ˜¯ç”±äºåˆå§‹åŒ–ä¸å¤Ÿå¥½å¯¼è‡´çš„ï¼Œèµ·åˆé€‰ç”¨çš„æ˜¯å›ºå®šé€‰ç‚¹ï¼Œåç»­ä½¿ç”¨çš„æ˜¯éšæœºæ ·æœ¬é€‰ç‚¹ï¼Œæ›´å¥½çš„åšæ³•åº”è¯¥æ˜¯ä½¿ç”¨Kmeansèšç±»é€‰æ‹©åˆå§‹åŒ–ç‚¹ã€‚
